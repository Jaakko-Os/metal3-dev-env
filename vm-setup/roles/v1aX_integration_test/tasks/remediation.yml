---
  - name: Get the master objects
    shell: "kubectl get m3m -n {{ NAMESPACE }} -o json |  jq -r '.items[] | \
            select(.metadata.name | contains(\"controlplane\")) | \
            (.metadata.annotations.\"metal3.io/BareMetalHost\"),([.metadata.ownerReferences |.[] | \
            select(.kind==\"Machine\")][0] | .name)'"
    register: masters

  - name: Get the worker objects
    shell: "kubectl get m3m -n {{ NAMESPACE }} -o json | jq -r '.items[] | \
            select(.metadata.name | contains(\"workers\")) | \
            (.metadata.annotations.\"metal3.io/BareMetalHost\"),([.metadata.ownerReferences |.[] | \
            select(.kind==\"Machine\")][0] | .name)'"
    register: workers

  - set_fact:
      WORKER_BMH: "{{ workers.stdout_lines.0 | replace('metal3/','')}}"
      WORKER_NODE: "{{ workers.stdout_lines.1 }}"
      MASTER_BMH_0: "{{ masters.stdout_lines.0 | replace('metal3/','')}}"
      MASTER_NODE_0: "{{ masters.stdout_lines.1 }}"
      MASTER_BMH_1: "{{ masters.stdout_lines.2 | replace('metal3/','')}}"
      MASTER_NODE_1: "{{ masters.stdout_lines.3 }}"
      MASTER_BMH_2: "{{ masters.stdout_lines.4 | replace('metal3/','')}}"
      MASTER_NODE_2: "{{ masters.stdout_lines.5 }}"
      MASTER_VM_0: "{{ masters.stdout_lines.0 | replace('-','_') | replace('metal3/','') }}"
      MASTER_VM_1: "{{ masters.stdout_lines.2 | replace('-','_') | replace('metal3/','') }}"
      MASTER_VM_2: "{{ masters.stdout_lines.4 | replace('-','_') | replace('metal3/','') }}"
      WORKER_VM: "{{ workers.stdout_lines.0 | replace('-','_') | replace('metal3/','') }}"

  - name: Fetch the target cluster kubeconfig
    shell: "kubectl get secrets {{ CLUSTER_NAME }}-kubeconfig -n {{ NAMESPACE }} -o json | jq -r '.data.value'| base64 -d > /tmp/kubeconfig-{{ CLUSTER_NAME }}.yaml"

  # Reboot a single worker node
  - name: Reboot "{{ WORKER_BMH }}"
    shell: |
       kubectl annotate bmh "{{ WORKER_BMH }}" -n "{{ NAMESPACE }}" reboot.metal3.io=

  - name: List only powered off VMs
    virt:
      command: list_vms
      state: shutdown
    register: shutdown_vms
    retries: 50
    delay: 10
    until: WORKER_VM in shutdown_vms.list_vms
    become: yes
    become_user: root

  - name: Wait until rebooted worker "{{ WORKER_NODE }}" becomes NotReady
    shell: "kubectl get nodes --kubeconfig /tmp/kubeconfig-{{ CLUSTER_NAME }}.yaml | grep -w NotReady | awk '{print $1}' | sort"
    retries: 150
    delay: 3
    register: not_ready_nodes
    until: WORKER_NODE in not_ready_nodes.stdout_lines

  - name: Wait until rebooted worker "{{ WORKER_NODE }}" becomes Ready
    shell: "kubectl get nodes --kubeconfig /tmp/kubeconfig-{{ CLUSTER_NAME }}.yaml | grep -w Ready | awk '{print $1}' | sort"
    retries: 150
    delay: 3
    register: ready_nodes
    until: WORKER_NODE in ready_nodes.stdout_lines

  - name: List only running VMs
    virt:
      command: list_vms
      state: running
    register: running_vms
    retries: 50
    delay: 10
    until: WORKER_VM in running_vms.list_vms
    become: yes
    become_user: root

  # Power cycle a single worker node
  - name: Power cycle a single worker node
    include: power_cycle.yml
    vars:
      BMH_NODE: "{{ WORKER_BMH }}"
      LIBVIRT_VM: "{{ WORKER_VM }}"
      K8S_NODE: "{{ WORKER_NODE }}"

  # Power cycle a single master node
  - name: Power cycle a single master node
    include: power_cycle.yml
    vars:
      BMH_NODE: "{{ MASTER_BMH_0 }}"
      LIBVIRT_VM: "{{ MASTER_VM_0 }}"
      K8S_NODE: "{{ MASTER_NODE_0 }}"

  # Power cycle two master nodes
  - name: Power off "{{ MASTER_BMH_1 }}" and "{{ MASTER_BMH_2 }}"
    shell: |
       kubectl annotate bmh "{{ item }}" -n "{{ NAMESPACE }}" reboot.metal3.io/poweroff=
    with_items:
      - "{{ MASTER_BMH_1 }}"
      - "{{ MASTER_BMH_2 }}"

  - pause:
     minutes: 1

  - name: List only powered off VMs
    virt:
      command: list_vms
      state: shutdown
    register: shutdown_vms
    retries: 50
    delay: 10
    until:
      - MASTER_VM_1 in shutdown_vms.list_vms
      - MASTER_VM_2 in shutdown_vms.list_vms
    become: yes
    become_user: root

  - name: Power on masters
    shell: |
       kubectl annotate bmh "{{ item }}" -n "{{ NAMESPACE }}" reboot.metal3.io/poweroff-
    with_items:
      - "{{ MASTER_BMH_1 }}"
      - "{{ MASTER_BMH_2 }}"

  - name: Wait until powered on master nodes become Ready
    shell: "kubectl get nodes --kubeconfig /tmp/kubeconfig-{{ CLUSTER_NAME }}.yaml | grep -w Ready | awk '{print $1}' | sort"
    register: ready_master
    retries: 150
    delay: 3
    until:
      - MASTER_NODE_1 in ready_master.stdout_lines
      - MASTER_NODE_2 in ready_master.stdout_lines

  - name: List only running VMs
    virt:
      command: list_vms
      state: running
    register: running_vms
    retries: 50
    delay: 10
    until:
      - MASTER_VM_1 in running_vms.list_vms
      - MASTER_VM_2 in running_vms.list_vms
    become: yes
    become_user: root

  # Test unhealthy annotations

  - name: Get KCP name
    shell: |
      kubectl get kcp -n "{{ NAMESPACE }}" -o json | jq -r '.items[]
      | .metadata.name'
    register: kcp_name

  - name: Define KCP_NAME variable
    set_fact:
      KCP_NAME: "{{ kcp_name.stdout }}"

  - name: Scale KCP down to two replicas
    shell: |
      kubectl scale kcp "{{ KCP_NAME }}" -n "{{ NAMESPACE }}" --replicas=2
  - name: Wait until KCP is scaled down and one node is Ready
    shell: kubectl get bmh -n "{{ NAMESPACE }}" | grep -w ready | wc -l
    retries: 200
    delay: 20
    register: ready_hosts
    until: ready_hosts.stdout == "1"

  - name: Get the name of a worker node
    shell: |
       kubectl get bmh -n "{{ NAMESPACE }}" -o json | jq -r '.items[]
       |select(.status.provisioning.state | contains("provisioned")) |select(.spec.consumerRef.name
       | contains("{{ CLUSTER_NAME }}-workers")) | .metadata.name'
    register: worker_node_name

  - name: Define WORKER_NODE variable with worker node name
    set_fact:
      WORKER_NODE: "{{ worker_node_name.stdout }}"

  - name: Mark "{{ WORKER_NODE }}" unhealthy
    shell: |
      kubectl annotate bmh "{{ WORKER_NODE }}" -n "{{ NAMESPACE }}" capi.metal3.io/unhealthy=
  - name: Get metal3machine name
    shell: |
      kubectl get bmh -n "{{ NAMESPACE }}" -o json | jq -r '.items[]
      | select (.metadata.name == "{{ WORKER_NODE }}")
      | .spec.consumerRef.name'
    register: metal3machine_name

  - name: Define M3M_NAME variable
    set_fact:
      M3M_NAME: "{{ metal3machine_name.stdout }}"

  - name: Get machine name
    shell: |
      kubectl get m3m -n "{{ NAMESPACE }}" -o json | jq -r '.items[]
      | select (.metadata.name == "{{ M3M_NAME }}")
      | .metadata.ownerReferences[].name'
    register: machine_name

  - name: Define MACHINE_NAME variable
    set_fact:
      MACHINE_NAME: "{{ machine_name.stdout }}"

  - name: Delete Machine object
    shell: |
      kubectl delete machine "{{ MACHINE_NAME }}" -n "{{ NAMESPACE }}"
  - name: Wait one BMH is in Ready state
    shell: kubectl get bmh -n "{{ NAMESPACE }}" | grep -w ready | wc -l
    retries: 200
    delay: 20
    register: ready_hosts
    until: ready_hosts.stdout == "1"

  - name: Wait until three BMH are provisioned
    shell: kubectl get bmh -n "{{ NAMESPACE }}" | grep -w provisioned | wc -l
    retries: 200
    delay: 20
    register: provisioned_hosts
    until: provisioned_hosts.stdout == "3"

  - name: Get machinedeployment name
    shell: |
      kubectl get machinedeployment -n metal3 -o json | jq -r '.items[]
      | .metadata.name'
    register: machinedeployment_name

  - name: Define MD_NAME variable
    set_fact:
      MD_NAME: "{{ machinedeployment_name.stdout }}"

  - name: Scale up the machinedeployment
    shell: kubectl scale machinedeployment "{{ MD_NAME }}" -n "{{ NAMESPACE }}" --replicas=2

  - name: Wait and verify that none of the nodes will not start provisioning
    shell: kubectl get bmh -n "{{ NAMESPACE }}" | grep -w provisioning | wc -l
    retries: 2
    delay: 60
    register: provisioning_hosts
    until: provisioning_hosts.stdout == "0"

  - name: Remove unhealthy annotation on "{{ WORKER_NODE }}"
    shell: |
      kubectl annotate bmh "{{ WORKER_NODE }}" -n "{{ NAMESPACE }}" capi.metal3.io/unhealthy-
    when: provisioning_hosts.stdout == "0"

  - name: Wait until all four BMH are provisioned
    shell: kubectl get bmh -n "{{ NAMESPACE }}" | grep -w provisioned | wc -l
    retries: 200
    delay: 20
    register: provisioned_hosts
    until: provisioned_hosts.stdout == "4"


  - name: Scale down the machinedeployment
    shell: kubectl scale machinedeployment "{{ MD_NAME }}" -n "{{ NAMESPACE }}" --replicas=1

  - name: Wait one BMH is in Ready state
    shell: kubectl get bmh -n "{{ NAMESPACE }}" | grep -w ready | wc -l
    retries: 200
    delay: 20
    register: ready_hosts
    until: ready_hosts.stdout == "1"

  - name: Scale KCP back to three replicas
    shell: |
      kubectl scale kcp "{{ KCP_NAME }}" -n "{{ NAMESPACE }}" --replicas=3
  - name: Wait until all four BMH are provisioned
    shell: kubectl get bmh -n "{{ NAMESPACE }}" | grep -w provisioned | wc -l
    retries: 200
    delay: 20
    register: provisioned_hosts
    until: provisioned_hosts.stdout == "4"
